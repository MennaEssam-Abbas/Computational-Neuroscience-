{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2rdmz8WB2_M7"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bPtiV5kl3EQN"
   },
   "outputs": [],
   "source": [
    "words = [\"I\", \"love\", \"deep\", \"learning\"]\n",
    "word_to_idx = {word: idx for idx, word in enumerate(words)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gCjdr5K63IGu"
   },
   "outputs": [],
   "source": [
    "X_data = [\"I\", \"love\", \"deep\"]\n",
    "Y_data = \"learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CoiXTfPx3MwX"
   },
   "outputs": [],
   "source": [
    "def one_hot(idx, size):\n",
    "    vec = np.zeros(size)\n",
    "    vec[idx] = 1\n",
    "    return vec\n",
    "\n",
    "vocab_size = len(words)\n",
    "\n",
    "X = np.array([one_hot(word_to_idx[word], vocab_size) for word in X_data])\n",
    "Y = word_to_idx[Y_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gWkUwfvS3RTN"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "hidden_size = 8\n",
    "\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01  # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01  # hidden to output\n",
    "\n",
    "# Biases\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5XGoMvDF3U_S"
   },
   "outputs": [],
   "source": [
    "def rnn_forward(X):\n",
    "    h_prev = np.zeros((hidden_size, 1))\n",
    "    hs = {}\n",
    "    hs[-1] = h_prev\n",
    "\n",
    "\n",
    "    for t in range(len(X)):\n",
    "        x_t = X[t].reshape(-1, 1)\n",
    "        h_t = np.tanh(np.dot(Wxh, x_t) + np.dot(Whh, h_prev) + bh)\n",
    "        hs[t] = h_t\n",
    "        h_prev = h_t\n",
    "\n",
    "\n",
    "    y_pred = np.dot(Why, h_t) + by\n",
    "    return y_pred, hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EVq5rNbL3YhU"
   },
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    exp_y = np.exp(y - np.max(y))\n",
    "    return exp_y / np.sum(exp_y)\n",
    "\n",
    "def cross_entropy(pred, target_idx):\n",
    "    return -np.log(pred[target_idx, 0] + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "CRoKDdBO3beb"
   },
   "outputs": [],
   "source": [
    "def rnn_backward(X, hs, y_pred, target_idx):\n",
    "\n",
    "    dWxh = np.zeros_like(Wxh)\n",
    "    dWhh = np.zeros_like(Whh)\n",
    "    dWhy = np.zeros_like(Why)\n",
    "    dbh = np.zeros_like(bh)\n",
    "    dby = np.zeros_like(by)\n",
    "\n",
    "\n",
    "    dy = softmax(y_pred)\n",
    "    dy[target_idx] -= 1\n",
    "\n",
    "    dWhy += np.dot(dy, hs[len(X)-1].T)\n",
    "    dby += dy\n",
    "\n",
    "    # Backpropagate into RNN\n",
    "    dh_next = np.zeros_like(hs[0])\n",
    "\n",
    "    for t in reversed(range(len(X))):\n",
    "        dh = np.dot(Why.T, dy) + dh_next\n",
    "        dh_raw = (1 - hs[t] ** 2) * dh\n",
    "\n",
    "        dWxh += np.dot(dh_raw, X[t].reshape(1, -1))\n",
    "        dWhh += np.dot(dh_raw, hs[t-1].T)\n",
    "        dbh += dh_raw\n",
    "\n",
    "        dh_next = np.dot(Whh.T, dh_raw)\n",
    "\n",
    "\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "\n",
    "    return dWxh, dWhh, dWhy, dbh, dby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "T5Ne_t553hO7"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "n_epochs = 500\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward\n",
    "    y_pred, hs = rnn_forward(X)\n",
    "    probs = softmax(y_pred)\n",
    "    loss = cross_entropy(probs, Y)\n",
    "\n",
    "    # Backward\n",
    "    dWxh, dWhh, dWhy, dbh, dby = rnn_backward(X, hs, y_pred, Y)\n",
    "\n",
    "    # Update parameters\n",
    "    Wxh -= learning_rate * dWxh\n",
    "    Whh -= learning_rate * dWhh\n",
    "    Why -= learning_rate * dWhy\n",
    "    bh -= learning_rate * dbh\n",
    "    by -= learning_rate * dby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67Cd70Rn3lVU",
    "outputId": "ff030c9a-8504-4ff9-f67e-f461a73a42aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss: 0.0019, Prediction: learning\n"
     ]
    }
   ],
   "source": [
    "if (epoch+1) % 50 == 0:\n",
    "        pred_idx = np.argmax(probs)\n",
    "        pred_word = idx_to_word[pred_idx]\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}, Prediction: {pred_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEr1a5K33nT8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
